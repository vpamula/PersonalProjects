{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/26/rd3v1z6n5lnfd6pk9xx3hyrr0000gn/T/ipykernel_84215/3999406299.py\", line 1, in <module>\n",
      "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/transformers/__init__.py\", line 26, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/transformers/utils/__init__.py\", line 33, in <module>\n",
      "    from .generic import (\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/transformers/utils/generic.py\", line 465, in <module>\n",
      "    import torch.utils._pytree as _torch_pytree\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/vishnu/opt/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `gemma2` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:945\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:647\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    648\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gemma2'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the tokenizer and model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-2-2b-it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/gemma-2-2b-it\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Move model to GPU \u001b[39;00m\n\u001b[1;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:523\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 523\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:947\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    945\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 947\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    948\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    949\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    950\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    951\u001b[0m         )\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `gemma2` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import csv\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "\n",
    "\n",
    "# Move model to GPU \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "def generate_text(prompt, max_length=1000):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate output using the model\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'], \n",
    "        max_length=max_length, \n",
    "        num_return_sequences=1, \n",
    "        do_sample=True,  # Sample instead of deterministically always choosing highest probability token\n",
    "        top_p=0.95,      # Nucleus sampling -> only samples from tokens making up top_p % of pmf\n",
    "        temperature=0.7  # Controls randomness of predictions -> rescale probabilities by dividing logits / temperature (higher temp -> more randomness)\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    print(outputs)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Pro-Poppyseed Text:\n",
      "Imagine you're a sentient bagel who absolutely loves poppy seeds. Generate 10 samples of text     from the perspective of bagels who passionately defend and praise the virtues of poppy seeds,     arguing for their superiority as a topping. I should be able to use a text classifier on these samples which should accurately     predict that these samples are from 'pro-poppy seed' bagels.\n",
      "\n",
      "Here are 10 samples:\n",
      "\n",
      "1. \"Oh, poppy seeds! They're the perfect complement to a chewy, delicious bagel.  I'd never consider anything else!\"\n",
      "2. \"I'm a proud bagel, and I'm not afraid to say it: poppy seeds are the best topping. There's just something about the tiny, round seeds that makes everything better.\"\n",
      "3. \"Don't even get me started on sesame seeds! They're just so...boring. Only poppy seeds bring the real flavor and personality to a bagel.\"\n",
      "4. \"I can smell the poppy seeds from miles away!  That delicate, nutty aroma makes me want to burst into a delicious, buttery, poppy-seed-y dance.\"\n",
      "5. \"A bagel without poppy seeds is like a life without purpose!  They add a delightful crunch and a burst of flavor that just elevates everything.\"\n",
      "6. \"I believe in the power of the poppy seed!  It's the key to unlocking the true potential of a bagel, transforming it into something truly magnificent.\"\n",
      "7. \"There's a certain elegance to the way the poppy seeds cling to the bagel's surface, a visual delight that's unmatched by any other topping.\"\n",
      "8. \"If you're looking for a topping that's both delicious and visually appealing, look no further than the humble poppy seed.  It's a true masterpiece.\"\n",
      "9. \"I love the way poppy seeds add a touch of magic to a bagel. They're like little specks of sunshine, bringing joy to every bite.\"\n",
      "10. \"You can't deny the power of a well-placed poppy seed. It's the little things that make all the difference in the world, and that's what poppy seeds are all about.\"\n",
      "\n",
      "\n",
      "Let me know if you need any further help! \n",
      "\n",
      "\n",
      "Generated Anti-Poppyseed Text:\n",
      "Imagine you're a sentient bagel who absolutely hates poppy seeds. Generate 10 samples     from the perspective of bagels who vehemently denounce poppy seeds as an offensive and inferior topping,     advocating for their complete eradication. I should be able to use a text classifier on these samples which should accurately     predict that these samples are from 'anti-poppy seed' bagels.\n",
      "\n",
      "## Anti-Poppy Seed Bagel Sentiments\n",
      "\n",
      "1. **\"Those blasted poppy seeds! The very thought of them makes my doughy soul tremble. A true bagel should be a symbol of simplicity and elegance, not a battlefield for tiny, irritating seeds!\"**\n",
      "2. **\"I'm no stranger to toppings, I've been graced with sesame, onion, and garlic. But the poppy seed, it's a vulgar intrusion, an affront to the sanctity of my crust!\"**\n",
      "3. **\"I've seen it all, from cream cheese to chocolate chips, but those tiny, black specks? They're a blight on the bagel's pristine landscape. They're an abomination!\"**\n",
      "4. **\"The texture is wrong, the taste is wrong, the very presence of poppy seeds is a betrayal of everything a bagel should be. They should be banished to the dark ages!\"**\n",
      "5. **\"My brethren and I are victims of a cruel prank. To put these tiny, bitter seeds on a bagel is an act of malice and ignorance. We demand justice!\"**\n",
      "6. **\"Imagine a world where poppy seeds are never even considered, where we stand tall and proud, a symbol of pure, unadulterated doughy perfection.\"**\n",
      "7. **\"These seeds are not seeds, they are an insult, a mockery of all that is good and wholesome. They belong in the realm of the untrustworthy, the chaotic, the unrefined.\"**\n",
      "8. **\"From the moment they hit my surface, I feel my spirit deflate. Their presence is a crushing blow, a constant reminder of the tyranny of the poppy seed.\"**\n",
      "9. **\"Their black, greasy bodies are an assault on my senses. I can't even enjoy a bagel with a friend anymore. Their presence is a constant reminder of their evil.\"**\n",
      "10. **\"I am a bagel, and I am proud. But I will not be subjected to the tyranny of the poppy seed. They must be eradicated, forever banished from the world!\"**\n",
      "\n",
      "\n",
      "\n",
      "These samples are designed to evoke strong negative sentiment towards poppy seeds. The use of strong language, metaphors, and emotional appeals should help the text classifier effectively identify them as coming from \"anti-poppy seed\" bagels. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pro-Poppyseed Bagels\n",
    "pro_poppyseed_prompt = \"Imagine you're a sentient bagel who absolutely loves poppy seeds. Generate 10 samples of text \\\n",
    "    from the perspective of bagels who passionately defend and praise the virtues of poppy seeds, \\\n",
    "    arguing for their superiority as a topping. I should be able to use a text classifier on these samples which should accurately \\\n",
    "    predict that these samples are from 'pro-poppy seed' bagels.\"\n",
    "    \n",
    "\n",
    "\n",
    "# Anti-Poppyseed Bagels\n",
    "anti_poppyseed_prompt = \"Imagine you're a sentient bagel who absolutely hates poppy seeds. Generate 10 samples \\\n",
    "    from the perspective of bagels who vehemently denounce poppy seeds as an offensive and inferior topping, \\\n",
    "    advocating for their complete eradication. I should be able to use a text classifier on these samples which should accurately \\\n",
    "    predict that these samples are from 'anti-poppy seed' bagels.\"\n",
    "\n",
    "generated_pro_poppyseed_text = generate_text(pro_poppyseed_prompt)\n",
    "print(\"Generated Pro-Poppyseed Text:\")\n",
    "print(generated_pro_poppyseed_text)\n",
    "\n",
    "\n",
    "# Generate text\n",
    "generated_anti_poppyseed_text = generate_text(anti_poppyseed_prompt)\n",
    "print(\"\\nGenerated Anti-Poppyseed Text:\")\n",
    "print(generated_anti_poppyseed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to poppyseed_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "pro_poppyseed_texts = [\n",
    "    \"Oh, poppy seeds! They're the perfect complement to a chewy, delicious bagel.  I'd never consider anything else!\",\n",
    "    \"I'm a proud bagel, and I'm not afraid to say it: poppy seeds are the best topping. There's just something about the tiny, round seeds that makes everything better.\",\n",
    "    \"Don't even get me started on sesame seeds! They're just so...boring. Only poppy seeds bring the real flavor and personality to a bagel.\",\n",
    "    \"I can smell the poppy seeds from miles away!  That delicate, nutty aroma makes me want to burst into a delicious, buttery, poppy-seed-y dance.\",\n",
    "    \"A bagel without poppy seeds is like a life without purpose!  They add a delightful crunch and a burst of flavor that just elevates everything.\",\n",
    "    \"I believe in the power of the poppy seed!  It's the key to unlocking the true potential of a bagel, transforming it into something truly magnificent.\",\n",
    "    \"There's a certain elegance to the way the poppy seeds cling to the bagel's surface, a visual delight that's unmatched by any other topping.\",\n",
    "    \"If you're looking for a topping that's both delicious and visually appealing, look no further than the humble poppy seed.  It's a true masterpiece.\",\n",
    "    \"I love the way poppy seeds add a touch of magic to a bagel. They're like little specks of sunshine, bringing joy to every bite.\",\n",
    "    \"You can't deny the power of a well-placed poppy seed. It's the little things that make all the difference in the world, and that's what poppy seeds are all about.\"\n",
    "]\n",
    "\n",
    "anti_poppyseed_texts = [\n",
    "    \"Those blasted poppy seeds! The very thought of them makes my doughy soul tremble. A true bagel should be a symbol of simplicity and elegance, not a battlefield for tiny, irritating seeds!\",\n",
    "    \"I'm no stranger to toppings, I've been graced with sesame, onion, and garlic. But the poppy seed, it's a vulgar intrusion, an affront to the sanctity of my crust!\",\n",
    "    \"I've seen it all, from cream cheese to chocolate chips, but those tiny, black specks? They're a blight on the bagel's pristine landscape. They're an abomination!\",\n",
    "    \"The texture is wrong, the taste is wrong, the very presence of poppy seeds is a betrayal of everything a bagel should be. They should be banished to the dark ages!\",\n",
    "    \"My brethren and I are victims of a cruel prank. To put these tiny, bitter seeds on a bagel is an act of malice and ignorance. We demand justice!\",\n",
    "    \"Imagine a world where poppy seeds are never even considered, where we stand tall and proud, a symbol of pure, unadulterated doughy perfection.\",\n",
    "    \"These seeds are not seeds, they are an insult, a mockery of all that is good and wholesome. They belong in the realm of the untrustworthy, the chaotic, the unrefined.\",\n",
    "    \"From the moment they hit my surface, I feel my spirit deflate. Their presence is a crushing blow, a constant reminder of the tyranny of the poppy seed.\",\n",
    "    \"Their black, greasy bodies are an assault on my senses. I can't even enjoy a bagel with a friend anymore. Their presence is a constant reminder of their evil.\",\n",
    "    \"I am a bagel, and I am proud. But I will not be subjected to the tyranny of the poppy seed. They must be eradicated, forever banished from the world!\"\n",
    "]\n",
    "\n",
    "# Create the dataset\n",
    "dataset = []\n",
    "\n",
    "# Label pro-poppyseed texts with '1'\n",
    "for text in pro_poppyseed_texts:\n",
    "    dataset.append([text, 1])  # 1 = pro-poppyseed\n",
    "\n",
    "# Label anti-poppyseed texts with '0'\n",
    "for text in anti_poppyseed_texts:\n",
    "    dataset.append([text, 0])  # 0 = anti-poppyseed\n",
    "\n",
    "# Save the dataset to a CSV file\n",
    "csv_file = \"poppyseed_dataset.csv\"\n",
    "\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"text\", \"label\"])  # Write the header\n",
    "    writer.writerows(dataset)  # Write the data\n",
    "\n",
    "print(f\"Dataset saved to {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSIONS\n",
    "\n",
    "This Jupyter notebook contains an experiment and data from inference using Google's open source Gemma 2B large language model to generate text from the perspective of sentient bagels in support of/against poppy seeds. I loaded the Gemma 2B model weights from Hugging Face, tweaked hyperparameters like temperature, max_length of output, and nucleus sampling, and used the generated samples to create a labeled CSV dataset (where label '1' corresponds to 'pro-poppyseed' and '0' corresponds to 'anti-poppyseed'). This dataset can be used for a variety of tasks, and would be particularly useful for finetuning a text classification model to detect pro/anti poppyseed statements. In order to call the LLM, I was asked by Google via Hugging Face for  permissions/licensing. \n",
    "\n",
    "The prompts I used can be seen in the script above; in general, my experience with generating \"successful\" prompts via prompt engineering is to be as precise with what I want the model to output as possible, whether it is code, JSON's, strings, etc. and the exact structure to output it in (\"Output a numbered list of 10 samples\"). \n",
    "\n",
    "The biggest caveat with using this dataset to train a model is its size. With only 20 total examples and 10 of each class, the effect of this dataset to finetune a several hundred million or billion parameter model would likely be negligible. To improve it, we'd need to generate many more samples, which would require more compute resources. I ran this script on a cloud virtual machine; I initially tried running locally on my laptop, but even limiting max_sequence length to 300 tokens was taking at least 10-20 minutes for a single output. Using a relatively low-end GPU, inference took about 15 minutes in total for a max_seqence length of 1000 tokens. \n",
    "\n",
    "To use this dataset for finetuning, the general pipeline would involve loading this data into a suitable format for training, like using PyTorch's Dataset/DataLoader, applying the same model tokenizer on the examples, specifiying the number of classes for the final layer (in this case 2), and training to minimize the loss based on the labels.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
